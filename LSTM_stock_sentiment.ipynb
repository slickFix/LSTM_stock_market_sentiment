{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Stock Market Sentiment with LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/siddharth/workspace-python/LSTM_stock_market_sentiment'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data\n",
    "\n",
    "\n",
    "Data used here is from **StockTwits.com** which is a social media network for traders and investors to share their views about the stock market. When a user posts a message, they tag the relevant stock ticker [$SPY in our case which is for S&P 500 index fund] and have option to tag the message with their sentiment - \"bullish\" or \"bearish\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "data = pd.read_csv('StockTwits_SPY_Sentiment_2017.gz',encoding='utf-8',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$SPY crazy day so far!</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$SPY Will make a new ATH this week. Watch it!</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$SPY $DJIA white elephant in room is $AAPL. Up 14% since election. Strong headwinds w/Trump trade &amp; Strong dollar. How many 7's do you see?</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SPY blocks above. We break above them We should push to double top</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$SPY Nothing happening in the market today, guess I'll go to the store and spend some $.</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       message  \\\n",
       "0                                                                                                                       $SPY crazy day so far!   \n",
       "1                                                                                                $SPY Will make a new ATH this week. Watch it!   \n",
       "2  $SPY $DJIA white elephant in room is $AAPL. Up 14% since election. Strong headwinds w/Trump trade & Strong dollar. How many 7's do you see?   \n",
       "3                                                                          $SPY blocks above. We break above them We should push to double top   \n",
       "4                                                     $SPY Nothing happening in the market today, guess I'll go to the store and spend some $.   \n",
       "\n",
       "  sentiment  \n",
       "0   bearish  \n",
       "1   bullish  \n",
       "2   bearish  \n",
       "3   bullish  \n",
       "4   bearish  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining text messages and their labels\n",
    "\n",
    "messages = data.message.values\n",
    "labels = data.sentiment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess messages\n",
    "\n",
    "Preprocessing the raw text data to normalize for the context. Normalizing for known unique 'entities' that carry similar contextual meaning. \n",
    "\n",
    "Therefore replacing the references to \n",
    "* specific stock ticker ($SPY), \n",
    "* user names, \n",
    "* url links,\n",
    "* numbers with special tokenidentifying the entity \n",
    "\n",
    "Converting text into lower case and removing punctuations.               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_messages(text):\n",
    "    \n",
    "    \n",
    "    # SAVING REGEX PATTERNS\n",
    "    REGEX_PRICE_SIGN = re.compile(r'\\$(?!\\d*\\.?\\d+%)\\d*\\.?\\d+|(?!\\d*\\.?\\d+%)\\d*\\.?\\d+\\$')\n",
    "    REGEX_PRICE_NOSIGN = re.compile(r'(?!\\d*\\.?\\d+%)(?!\\d*\\.?\\d+k)\\d*\\.?\\d+')\n",
    "    REGEX_TICKER = re.compile('\\$[a-zA-Z]+')\n",
    "    REGEX_USER = re.compile('\\@\\w+')\n",
    "    REGEX_LINK = re.compile('https?:\\/\\/[^\\s]+')\n",
    "    REGEX_HTML_ENTITY = re.compile('\\&\\w+')\n",
    "    REGEX_NON_ACSII = re.compile('[^\\x00-\\x7f]')\n",
    "    \n",
    "    #string.punctuation - '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    #string.punctuation.replace('<', '').replace('>', '')\n",
    "    #--> '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~'\n",
    "    #re.escape(string.punctuation.replace('<', ''))\n",
    "    #--> '\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~'\n",
    "    \n",
    "    REGEX_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation.replace('<', '').replace('>', '')))\n",
    "    REGEX_NUMBER = re.compile(r'[-+]?[0-9]+')\n",
    "    \n",
    "    \n",
    "    # CONVERTING TO LOWERCASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    # REPLACE ST \"ENTITITES\" WITH A UNIQUE TOKEN\n",
    "    text = re.sub(REGEX_TICKER, ' <TICKER> ', text)\n",
    "    text = re.sub(REGEX_USER, ' <USER> ', text)\n",
    "    text = re.sub(REGEX_LINK, ' <LINK> ', text)\n",
    "    text = re.sub(REGEX_PRICE_SIGN, ' <PRICE> ', text)\n",
    "    text = re.sub(REGEX_PRICE_NOSIGN, ' <NUMBER> ', text)\n",
    "    text = re.sub(REGEX_NUMBER, ' <NUMBER> ', text)\n",
    "    # REMOVE EXTRANEOUS TEXT DATA\n",
    "    text = re.sub(REGEX_HTML_ENTITY, \"\", text)\n",
    "    text = re.sub(REGEX_NON_ACSII, \"\", text)\n",
    "    text = re.sub(REGEX_PUNCTUATION, \"\", text)\n",
    "    \n",
    "    # Tokenizing and removing < and > that are not in special tokens\n",
    "    words = \" \".join(token.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "                     if token not in ['<TICKER>', '<USER>', '<LINK>', '<PRICE>', '<NUMBER>']\n",
    "                     else token\n",
    "                     for token in text.split())\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = np.array([preprocess_messages(msg) for msg in messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Vocab to index mapping\n",
    "\n",
    "Encoding words to numbers for the alogrithm to work with inputs by encoding each word to a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \" \".join(messages).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267980"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31980"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = {word:idx for idx,word in enumerate(set(vocab),1)}\n",
    "idx_word = {idx:word for word,idx in word_idx.items()}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking messages length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length :  0\n",
      "Maximum length :  244\n",
      "Mean length :  78.21856920395598\n"
     ]
    }
   ],
   "source": [
    "message_len = [len(msg) for msg in messages]\n",
    "\n",
    "print('Minimum length : ',min(message_len))\n",
    "print('Maximum length : ',max(message_len))\n",
    "print('Mean length : ',np.mean(message_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes where message length is 0 : [88808]\n"
     ]
    }
   ],
   "source": [
    "min_idx = [i  for i in range(len(message_len)) if message_len[i]==0]\n",
    "print(\"Indexes where message length is 0 :\",min_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages length:  96967\n",
      "no of labels:  96967\n"
     ]
    }
   ],
   "source": [
    "print('messages length: ',len(messages))\n",
    "print('no of labels: ',len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping zero message length message\n",
    "\n",
    "messages = np.delete(messages,min_idx)\n",
    "labels = np.delete(labels,min_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages length after removing of zero length messages:  96966\n",
      "no of labels after removing of zero length messages:  96966\n"
     ]
    }
   ],
   "source": [
    "print('messages length after removing of zero length messages: ',len(messages))\n",
    "print('no of labels after removing of zero length messages: ',len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Messages and Labels to the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_messages(messages,word_idx):\n",
    "    encoded_msg = [] \n",
    "    for msg in messages:\n",
    "        encoded_msg.append([word_idx[word] for word in msg.split()])\n",
    "    \n",
    "    return np.array(encoded_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([17858, 30285, 6829, 26065, 20189]),\n",
       "       list([17858, 18370, 25216, 6375, 14256, 22919, 29976, 13677, 22708, 23702]),\n",
       "       list([17858, 17858, 4375, 19528, 7484, 14494, 29522, 17858, 28876, 30517, 28674, 7408, 13300, 23429, 24283, 29102, 13300, 26201, 24964, 5876, 30517, 26156, 29142, 25049, 7531]),\n",
       "       ..., list([17858, 28750, 904, 8568, 10131, 5485, 7064]),\n",
       "       list([17858, 30517, 13998, 26218, 6504, 29522, 4242, 8161, 8481, 1164]),\n",
       "       list([17858, 15888, 29522, 6375, 6829, 12216, 4394, 10337, 26886, 25049, 27474, 6375, 30517, 29397, 14584, 18855, 25049, 8617, 18328, 12718, 4734, 19383, 29976, 25165, 17858, 17858, 17858])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_msg = encode_messages(messages,word_idx)\n",
    "encoded_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bullish    53704\n",
       "bearish    43263\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    return np.array([0 if label=='bullish' else 1 for label in labels ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_label = encode_labels(labels)\n",
    "encoded_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Padding the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the maximum sentence\n",
    "\n",
    "#len_encoded_msg = [len(i) for i in encoded_msg]\n",
    "#seq_len = max(len_encoded_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = max(message_len)\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the encoded_messages\n",
    "\n",
    "padd_msg = np.zeros((len(encoded_msg),seq_len))\n",
    "\n",
    "for i,message in enumerate(encoded_msg):\n",
    "    padd_msg[i,seq_len-len(message):] = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96966, 244)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padd_msg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train,Test,Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating x and test split\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(padd_msg, encoded_label, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x :  (87269, 244)\n",
      "Shape of y :  (87269,)\n",
      "Shape of x_test set :  (9697, 244)\n",
      "Shape of y_test set :  (9697,)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the respective sets\n",
    "\n",
    "print(\"Shape of x : \",x.shape)\n",
    "print(\"Shape of y : \",y.shape)\n",
    "print(\"Shape of x_test set : \",x_test.shape)\n",
    "print(\"Shape of y_test set : \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and validation split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train :  (78542, 244)\n",
      "Shape of y_train :  (78542,)\n",
      "Shape of x_val set :  (8727, 244)\n",
      "Shape of y_val set :  (8727,)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the respective sets\n",
    "\n",
    "print(\"Shape of x_train : \",x_train.shape)\n",
    "print(\"Shape of y_train : \",y_train.shape)\n",
    "print(\"Shape of x_val set : \",x_val.shape)\n",
    "print(\"Shape of y_val set : \",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    'creating placeholders'\n",
    "    \n",
    "    with tf.variable_scope('Placeholders'):\n",
    "        x_ph = tf.placeholder(tf.int32,[None,None],name ='x_ph')\n",
    "        y_ph = tf.placeholder(tf.int32,None, name='y_ph')\n",
    "        keep_prob_ph = tf.placeholder(tf.float32,name='keep_prob_ph')\n",
    "    \n",
    "    return x_ph,y_ph,keep_prob_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x_ph,vocab_size,embed_size,lstm_neurons_li,keep_prob_ph,batch_size):\n",
    "    \n",
    "    # creating embedding layer\n",
    "    with tf.variable_scope('Embedding_layer'):\n",
    "        embedding = tf.Variable(tf.random_uniform((vocab_size,embed_size),minval=-1,maxval=1))\n",
    "        embed_layer = tf.nn.embedding_lookup(embedding,x_ph)\n",
    "    \n",
    "    # creating LSTM layer\n",
    "    with tf.variable_scope('LSTM_layer'):\n",
    "        # creating lstm cells\n",
    "        lstms = [tf.contrib.rnn.BasicLSTMCell(size,name='lstm_cell') for size in lstm_neurons_li]\n",
    "        # adding dopout to the cells\n",
    "        drops = [tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob = keep_prob_ph) for lstm in lstms]\n",
    "        # stacking multiple LSTM layers\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "        \n",
    "        \n",
    "        # getting initial state of all zeros\n",
    "        init_state = cell.zero_state(batch_size,tf.float32)\n",
    "        #init_state = tf.identity(init_state, name=\"init_state\")\n",
    "        \n",
    "        lstm_outputs,final_state = tf.nn.dynamic_rnn(cell,embed_layer,initial_state=init_state)\n",
    "    \n",
    "    # creating sigmoid fc layer\n",
    "    with tf.variable_scope('FC_layer'):\n",
    "        a_output = tf.contrib.layers.fully_connected(lstm_outputs[:,-1],1,activation_fn=tf.sigmoid)\n",
    "    \n",
    "    return init_state,a_output,cell,final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a_output,y_ph):\n",
    "    \n",
    "    with tf.variable_scope('Loss'):\n",
    "        cost = tf.losses.mean_squared_error(y_ph,a_output)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "\n",
    "def acc_fn(a_output,y_ph):\n",
    "    \n",
    "    with tf.variable_scope('Accuracy'):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(a_output),tf.int32),y_ph)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x,y,batch_size = 100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    # removing left out records\n",
    "    x,y = x[:n_batches*batch_size],y[:n_batches*batch_size]\n",
    "    \n",
    "    for i in range(0,len(x),batch_size):\n",
    "        yield x[i:i+batch_size],y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_training\n",
    "\n",
    "def model_train(x_train,y_train,x_val,y_val,vocab_size,\n",
    "                embed_size=300,lstm_neurons_li=[128,64],\n",
    "                keep_prob=0.5,learning_rate=1e-1,epochs=50,batch_size=256):\n",
    "    \n",
    "    # reset default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # create placeholder\n",
    "    x_ph,y_ph,keep_prob_ph = create_placeholders()\n",
    "    \n",
    "    # forward propogation\n",
    "    init_state,a_output,cell,final_state = forward_propagation(x_ph,vocab_size,\n",
    "                                                                       embed_size,lstm_neurons_li,\n",
    "                                                                       keep_prob_ph,batch_size)    \n",
    "    \n",
    "    # cost calculation\n",
    "    cost = compute_cost(a_output,y_ph)\n",
    "    \n",
    "    # optimizers\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # accuracy definition\n",
    "    accuracy = acc_fn(a_output,y_ph)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(x_train)//batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            startime = datetime.now()\n",
    "            state = sess.run(init_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            for step,(x,y) in enumerate(get_batches(x_train,y_train,batch_size),1):\n",
    "                print('.',end=' ')\n",
    "                feed = {x_ph:x,\n",
    "                        y_ph:y,\n",
    "                        keep_prob_ph:keep_prob,\n",
    "                        init_state:state}\n",
    "                \n",
    "                loss_,state,_,batch_acc = sess.run([cost,final_state,optimizer,accuracy],feed_dict=feed)\n",
    "                train_acc.append(batch_acc)\n",
    "                \n",
    "                # after the last batch is used for training i.e. after every epoch of training, evaluating result\n",
    "                if (step)%n_batches == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    \n",
    "                    val_state = sess.run(cell.zero_state(batch_size,tf.float32))\n",
    "                    \n",
    "                    for xx,yy in get_batches(x_val,y_val,batch_size):\n",
    "                        feed_val = {x_ph:xx,\n",
    "                                y_ph:yy,\n",
    "                                keep_prob_ph:1,\n",
    "                                init_state:val_state}\n",
    "                        \n",
    "                        val_batch_acc,val_state = sess.run([accuracy,final_state],feed_dict=feed_val)\n",
    "                        val_acc.append(val_batch_acc)\n",
    "                    \n",
    "                    stoptime = datetime.now()\n",
    "                    print()\n",
    "                    print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                          \"Batch: {}/{}...\".format(step, n_batches),\n",
    "                          \"Train Loss: {:.3f}...\".format(loss_),\n",
    "                          \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "                          \"Val Accuracy: {:.3f}\".format(np.mean(val_acc)),\n",
    "                          \"Epoch time: {}\".format(str(stoptime-startime)))\n",
    "            \n",
    "            saver.save(sess,'./model_save1/sentiment.ckpt',global_step = epoch+1)\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "Epoch: 1/50... Batch: 306/306... Train Loss: 0.249... Train Accruacy: 0.532... Val Accuracy: 0.557 Epoch time: 0:12:58.607614\n",
      ". . . . . . . . . "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-55976e3221cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model_train(x_train,y_train,x_val,y_val,vocab_size,\n\u001b[1;32m      4\u001b[0m                 \u001b[0membed_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_neurons_li\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 keep_prob=0.5,learning_rate=1e-1,epochs=50,batch_size=256)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-8e290b7d3101>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(x_train, y_train, x_val, y_val, vocab_size, embed_size, lstm_neurons_li, keep_prob, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m                         init_state:state}\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_idx)+1\n",
    "\n",
    "model_train(x_train,y_train,x_val,y_val,vocab_size,\n",
    "                embed_size=300,lstm_neurons_li=[128,64],\n",
    "                keep_prob=0.5,learning_rate=1e-1,epochs=50,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model_dir,x_test,y_test,batch_size = 256):\n",
    "    \n",
    "    test_acc = []\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(model_dir+'.meta')\n",
    "        saver.restore(sess,model_dir)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "                \n",
    "        y_ph = graph.get_tensor_by_name('Placeholders/y_ph:0')\n",
    "        x_ph = graph.get_tensor_by_name('Placeholders/x_ph:0')\n",
    "        keep_prob_ph = graph.get_tensor_by_name('Placeholders/keep_prob_ph:0')\n",
    "        accuracy = graph.get_tensor_by_name('Accuracy/Mean:0')\n",
    "        \n",
    "        for i,(x,y) in enumerate(get_batches(x_test,y_test,batch_size),1):\n",
    "            feed_test = {x_ph:x,\n",
    "                         y_ph:y,\n",
    "                         keep_prob_ph:1}                         \n",
    "            \n",
    "            batch_acc = sess.run([accuracy],feed_dict=feed_test)\n",
    "            test_acc.append(batch_acc)\n",
    "        print(\"Test Accuracy : {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'model_save/sentiment.ckpt-1'\n",
    "model_test(model_dir,x_test,y_test,batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
