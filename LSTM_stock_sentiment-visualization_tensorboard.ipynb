{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Stock Market Sentiment with LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shivanand/Downloads/LSTM_stock_market_sentiment'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data\n",
    "\n",
    "\n",
    "Data used here is from **StockTwits.com** which is a social media network for traders and investors to share their views about the stock market. When a user posts a message, they tag the relevant stock ticker [$SPY in our case which is for S&P 500 index fund] and have option to tag the message with their sentiment - \"bullish\" or \"bearish\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "data = pd.read_csv('StockTwits_SPY_Sentiment_2017.gz',encoding='utf-8',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$SPY crazy day so far!</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$SPY Will make a new ATH this week. Watch it!</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$SPY $DJIA white elephant in room is $AAPL. Up 14% since election. Strong headwinds w/Trump trade &amp; Strong dollar. How many 7's do you see?</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SPY blocks above. We break above them We should push to double top</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$SPY Nothing happening in the market today, guess I'll go to the store and spend some $.</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       message  \\\n",
       "0                                                                                                                       $SPY crazy day so far!   \n",
       "1                                                                                                $SPY Will make a new ATH this week. Watch it!   \n",
       "2  $SPY $DJIA white elephant in room is $AAPL. Up 14% since election. Strong headwinds w/Trump trade & Strong dollar. How many 7's do you see?   \n",
       "3                                                                          $SPY blocks above. We break above them We should push to double top   \n",
       "4                                                     $SPY Nothing happening in the market today, guess I'll go to the store and spend some $.   \n",
       "\n",
       "  sentiment  \n",
       "0   bearish  \n",
       "1   bullish  \n",
       "2   bearish  \n",
       "3   bullish  \n",
       "4   bearish  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining text messages and their labels\n",
    "\n",
    "messages = data.message.values\n",
    "labels = data.sentiment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess messages\n",
    "\n",
    "Preprocessing the raw text data to normalize for the context. Normalizing for known unique 'entities' that carry similar contextual meaning. \n",
    "\n",
    "Therefore replacing the references to \n",
    "* specific stock ticker ($SPY), \n",
    "* user names, \n",
    "* url links,\n",
    "* numbers with special tokenidentifying the entity \n",
    "\n",
    "Converting text into lower case and removing punctuations.               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_messages(text):\n",
    "    \n",
    "    \n",
    "    # SAVING REGEX PATTERNS\n",
    "    REGEX_PRICE_SIGN = re.compile(r'\\$(?!\\d*\\.?\\d+%)\\d*\\.?\\d+|(?!\\d*\\.?\\d+%)\\d*\\.?\\d+\\$')\n",
    "    REGEX_PRICE_NOSIGN = re.compile(r'(?!\\d*\\.?\\d+%)(?!\\d*\\.?\\d+k)\\d*\\.?\\d+')\n",
    "    REGEX_TICKER = re.compile('\\$[a-zA-Z]+')\n",
    "    REGEX_USER = re.compile('\\@\\w+')\n",
    "    REGEX_LINK = re.compile('https?:\\/\\/[^\\s]+')\n",
    "    REGEX_HTML_ENTITY = re.compile('\\&\\w+')\n",
    "    REGEX_NON_ACSII = re.compile('[^\\x00-\\x7f]')\n",
    "    \n",
    "    #string.punctuation - '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    #string.punctuation.replace('<', '').replace('>', '')\n",
    "    #--> '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~'\n",
    "    #re.escape(string.punctuation.replace('<', ''))\n",
    "    #--> '\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~'\n",
    "    \n",
    "    REGEX_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation.replace('<', '').replace('>', '')))\n",
    "    REGEX_NUMBER = re.compile(r'[-+]?[0-9]+')\n",
    "    \n",
    "    \n",
    "    # CONVERTING TO LOWERCASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    # REPLACE ST \"ENTITITES\" WITH A UNIQUE TOKEN\n",
    "    text = re.sub(REGEX_TICKER, ' <TICKER> ', text)\n",
    "    text = re.sub(REGEX_USER, ' <USER> ', text)\n",
    "    text = re.sub(REGEX_LINK, ' <LINK> ', text)\n",
    "    text = re.sub(REGEX_PRICE_SIGN, ' <PRICE> ', text)\n",
    "    text = re.sub(REGEX_PRICE_NOSIGN, ' <NUMBER> ', text)\n",
    "    text = re.sub(REGEX_NUMBER, ' <NUMBER> ', text)\n",
    "    # REMOVE EXTRANEOUS TEXT DATA\n",
    "    text = re.sub(REGEX_HTML_ENTITY, \"\", text)\n",
    "    text = re.sub(REGEX_NON_ACSII, \"\", text)\n",
    "    text = re.sub(REGEX_PUNCTUATION, \"\", text)\n",
    "    \n",
    "    # Tokenizing and removing < and > that are not in special tokens\n",
    "    words = \" \".join(token.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "                     if token not in ['<TICKER>', '<USER>', '<LINK>', '<PRICE>', '<NUMBER>']\n",
    "                     else token\n",
    "                     for token in text.split())\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = np.array([preprocess_messages(msg) for msg in messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Vocab to index mapping\n",
    "\n",
    "Encoding words to numbers for the alogrithm to work with inputs by encoding each word to a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \" \".join(messages).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267980"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31980"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = {word:idx for idx,word in enumerate(set(vocab),1)}\n",
    "idx_word = {idx:word for word,idx in word_idx.items()}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking messages length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length :  0\n",
      "Maximum length :  244\n",
      "Mean length :  78.21856920395598\n"
     ]
    }
   ],
   "source": [
    "message_len = [len(msg) for msg in messages]\n",
    "\n",
    "print('Minimum length : ',min(message_len))\n",
    "print('Maximum length : ',max(message_len))\n",
    "print('Mean length : ',np.mean(message_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes where message length is 0 : [88808]\n"
     ]
    }
   ],
   "source": [
    "min_idx = [i  for i in range(len(message_len)) if message_len[i]==0]\n",
    "print(\"Indexes where message length is 0 :\",min_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages length:  96967\n",
      "no of labels:  96967\n"
     ]
    }
   ],
   "source": [
    "print('messages length: ',len(messages))\n",
    "print('no of labels: ',len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping zero message length message\n",
    "\n",
    "messages = np.delete(messages,min_idx)\n",
    "labels = np.delete(labels,min_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages length after removing of zero length messages:  96966\n",
      "no of labels after removing of zero length messages:  96966\n"
     ]
    }
   ],
   "source": [
    "print('messages length after removing of zero length messages: ',len(messages))\n",
    "print('no of labels after removing of zero length messages: ',len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Messages and Labels to the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_messages(messages,word_idx):\n",
    "    encoded_msg = [] \n",
    "    for msg in messages:\n",
    "        encoded_msg.append([word_idx[word] for word in msg.split()])\n",
    "    \n",
    "    return np.array(encoded_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([20265, 1186, 26058, 24964, 8775]),\n",
       "       list([20265, 12597, 7098, 24504, 5464, 13036, 23716, 31580, 30013, 9633]),\n",
       "       list([20265, 20265, 11855, 26267, 27238, 23235, 23530, 20265, 17343, 7862, 23669, 5197, 28385, 30354, 25974, 27204, 28385, 26935, 10680, 1014, 7862, 24285, 9641, 3814, 9243]),\n",
       "       ..., list([20265, 2853, 3267, 18484, 17227, 27942, 20978]),\n",
       "       list([20265, 7862, 27528, 29009, 13617, 23530, 7494, 7137, 13222, 1678]),\n",
       "       list([20265, 30345, 23530, 24504, 26058, 14779, 24321, 22020, 8873, 3814, 17199, 24504, 7862, 6952, 16991, 10451, 3814, 7180, 12793, 27116, 16167, 7384, 23716, 27263, 20265, 20265, 20265])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_msg = encode_messages(messages,word_idx)\n",
    "encoded_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bullish    53704\n",
       "bearish    43263\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    return np.array([0 if label=='bullish' else 1 for label in labels ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_label = encode_labels(labels)\n",
    "encoded_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Padding the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the maximum sentence\n",
    "\n",
    "# len_encoded_msg = [len(i) for i in encoded_msg]\n",
    "# seq_len1 = max(len_encoded_msg)\n",
    "# seq_len1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Minimum length : ',min(len_encoded_msg))\n",
    "# print('Maximum length : ',max(len_encoded_msg))\n",
    "# print('Mean length : ',np.mean(len_encoded_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_word_index = [index for index,sentence in enumerate(encoded_msg) if len(sentence)==1]\n",
    "# print('No of single word sentences :',len(one_word_index))\n",
    "# one_word_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20265]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_msg[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = max(message_len)\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the encoded_messages\n",
    "\n",
    "padd_msg = np.zeros((len(encoded_msg),seq_len))\n",
    "\n",
    "for i,message in enumerate(encoded_msg):\n",
    "    padd_msg[i,seq_len-len(message):] = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96966, 244)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padd_msg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.,     0.,     0., ..., 26058., 24964.,  8775.],\n",
       "       [    0.,     0.,     0., ..., 31580., 30013.,  9633.],\n",
       "       [    0.,     0.,     0., ...,  9641.,  3814.,  9243.],\n",
       "       ...,\n",
       "       [    0.,     0.,     0., ..., 17227., 27942., 20978.],\n",
       "       [    0.,     0.,     0., ...,  7137., 13222.,  1678.],\n",
       "       [    0.,     0.,     0., ..., 20265., 20265., 20265.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padd_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train,Test,Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating x and test split\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(padd_msg, encoded_label, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x :  (87269, 244)\n",
      "Shape of y :  (87269,)\n",
      "Shape of x_test set :  (9697, 244)\n",
      "Shape of y_test set :  (9697,)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the respective sets\n",
    "\n",
    "print(\"Shape of x : \",x.shape)\n",
    "print(\"Shape of y : \",y.shape)\n",
    "print(\"Shape of x_test set : \",x_test.shape)\n",
    "print(\"Shape of y_test set : \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and validation split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train :  (78542, 244)\n",
      "Shape of y_train :  (78542,)\n",
      "Shape of x_val set :  (8727, 244)\n",
      "Shape of y_val set :  (8727,)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the respective sets\n",
    "\n",
    "print(\"Shape of x_train : \",x_train.shape)\n",
    "print(\"Shape of y_train : \",y_train.shape)\n",
    "print(\"Shape of x_val set : \",x_val.shape)\n",
    "print(\"Shape of y_val set : \",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x,y,batch_size = 100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    # removing left out records\n",
    "    x,y = x[:n_batches*batch_size],y[:n_batches*batch_size]\n",
    "    \n",
    "    for i in range(0,len(x),batch_size):\n",
    "        yield x[i:i+batch_size],y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('G_Placeholders'):\n",
    "        inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "        labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        keep_prob_ = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs_, labels_, keep_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_layer(inputs_, vocab_size, embed_size):\n",
    "    \"\"\"\n",
    "    Create the embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope('G_Embedding_layer'):\n",
    "        embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size):\n",
    "    \"\"\"\n",
    "    Create the LSTM layers\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope('G_LSTM_layer'):\n",
    "        lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\n",
    "        # Add dropout to the cell\n",
    "        drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\n",
    "        # Stack up multiple LSTM layers, for deep learning\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "        # Getting an initial state of all zeros\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "    return initial_state, lstm_outputs, cell, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate):\n",
    "    \"\"\"\n",
    "    Create the Loss function and Optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope('G_FC_layer'):\n",
    "        predictions = tf.contrib.layers.fully_connected(lstm_outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "        \n",
    "        tf.summary.histogram('Predictions',predictions)\n",
    "    \n",
    "    with tf.variable_scope('G_Loss'):\n",
    "        loss = tf.losses.mean_squared_error(labels_, predictions)\n",
    "        \n",
    "        tf.summary.scalar(\"G_Loss\", loss)\n",
    "        \n",
    "        \n",
    "    optimzer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return predictions, loss, optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_accuracy(predictions, labels_):\n",
    "    \"\"\"\n",
    "    Create accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope('G_Accuracy'):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        tf.summary.scalar(\"G_Accuracy\", accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL FUNCTION IN ONE TRAINING_GARRETT\n",
    "\n",
    "def build_and_train_network_all_in_one(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train_x, val_x, train_y, val_y):\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()    \n",
    "    \n",
    "\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    \n",
    "    \n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    \n",
    "        \n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    \n",
    "    \n",
    "    \n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #[n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    \n",
    "    print(\"Printing all trainable tensor_names\".center(50,'-'))\n",
    "    \n",
    "    print()\n",
    "    print(\"LSTM WEIGHTS\")\n",
    "    print()\n",
    "    \n",
    "    [print(n.name)for n in tf.trainable_variables('G_LSTM_layer')]\n",
    "    \n",
    "    [tf.summary.histogram(n.name, n)for n in tf.trainable_variables('G_LSTM_layer')]\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"FC WIEGHTS\")\n",
    "    print()\n",
    "    \n",
    "    [print(n.name)for n in tf.trainable_variables('G_FC_layer')]\n",
    "    \n",
    "    [tf.summary.histogram(n.name, n)for n in tf.trainable_variables('G_FC_layer')]\n",
    "    \n",
    "    print()\n",
    "    print(\"tensor_names\".center(50,'-'))\n",
    "    \n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(train_x)//batch_size\n",
    "\n",
    "        writer_train = tf.summary.FileWriter('./tb/garrett_all_in_one/train/',sess.graph)\n",
    "        writer_val = tf.summary.FileWriter('./tb/garrett_all_in_one/val/')\n",
    "        \n",
    "        global_step = 0\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            state = sess.run(initial_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "                \n",
    "                global_step+=1\n",
    "                \n",
    "                print(ii,end=' ')\n",
    "                \n",
    "                feed = {inputs_: x,\n",
    "                        labels_: y[:,None],\n",
    "                        keep_prob_: keep_prob,\n",
    "                        initial_state: state}\n",
    "                loss_, state, _,  batch_acc,s = sess.run([loss, final_state, optimizer, accuracy,summ], feed_dict=feed)\n",
    "                \n",
    "                writer_train.add_summary(s,global_step) # writing summary real time\n",
    "                train_acc.append(batch_acc)\n",
    "                \n",
    "                if (ii) % 17 == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    #val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "                    \n",
    "                    for xx, yy in get_batches(val_x, val_y, batch_size):\n",
    "                        feed_val = {inputs_: xx,\n",
    "                                labels_: yy[:,None],\n",
    "                                keep_prob_: 1}  \n",
    "                            #initial_state: val_state}\n",
    "\n",
    "                        val_batch_acc,s = sess.run([accuracy, summ], feed_dict=feed_val)                                                                \n",
    "                        val_acc.append(val_batch_acc)\n",
    "                        \n",
    "                    writer_val.add_summary(s,global_step) # writing summary real time\n",
    "                    \n",
    "                \n",
    "                if (ii ) % n_batches == 0:                \n",
    "                    \n",
    "                    val_acc = []\n",
    "                    #val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "                    \n",
    "                    for xx, yy in get_batches(val_x, val_y, batch_size):\n",
    "                        feed_val = {inputs_: xx,\n",
    "                                labels_: yy[:,None],\n",
    "                                keep_prob_: 1}  \n",
    "                            #initial_state: val_state}\n",
    "\n",
    "                        val_batch_acc = sess.run([accuracy], feed_dict=feed_val)                                                                \n",
    "                        val_acc.append(val_batch_acc)\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "#                     feed_val = {inputs_: val_x,\n",
    "#                                 labels_: val_y[:,None],\n",
    "#                                 keep_prob_: 1}\n",
    "                    \n",
    "#                     val_batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    \n",
    "                    \n",
    "                    stop_time = datetime.now()\n",
    "                    \n",
    "                    print()\n",
    "                    print()\n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                          \"Batch: {}/{}...\".format(ii, n_batches),\n",
    "                          \"Train Loss: {:.3f}...\".format(loss_),\n",
    "                          \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "                          \"Val Accuracy: {:.3f}\".format(np.mean(val_acc)),\n",
    "                          \"Epoch time : {}\".format(str(stop_time-start_time)))\n",
    "                    \n",
    "                    print()\n",
    "                    print()\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inputs and Hyperparameters\n",
    "lstm_sizes = [128, 64]\n",
    "vocab_size = len(word_idx) + 1 #add one for padding\n",
    "embed_size = 300\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Printing all trainable tensor_names--------\n",
      "\n",
      "LSTM WEIGHTS\n",
      "\n",
      "G_LSTM_layer/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "G_LSTM_layer/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "G_LSTM_layer/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "G_LSTM_layer/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:Summary name G_LSTM_layer/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0 is illegal; using G_LSTM_layer/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name G_LSTM_layer/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0 is illegal; using G_LSTM_layer/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0 instead.\n",
      "INFO:tensorflow:Summary name G_LSTM_layer/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0 is illegal; using G_LSTM_layer/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name G_LSTM_layer/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0 is illegal; using G_LSTM_layer/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0 instead.\n",
      "\n",
      "\n",
      "FC WIEGHTS\n",
      "\n",
      "G_FC_layer/fully_connected/weights:0\n",
      "G_FC_layer/fully_connected/biases:0\n",
      "INFO:tensorflow:Summary name G_FC_layer/fully_connected/weights:0 is illegal; using G_FC_layer/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name G_FC_layer/fully_connected/biases:0 is illegal; using G_FC_layer/fully_connected/biases_0 instead.\n",
      "\n",
      "-------------------tensor_names-------------------\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 1/10... Batch: 306/306... Train Loss: 0.241... Train Accruacy: 0.548... Val Accuracy: 0.575 Epoch time : 0:03:37.066406\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 2/10... Batch: 306/306... Train Loss: 0.233... Train Accruacy: 0.578... Val Accuracy: 0.600 Epoch time : 0:03:36.346806\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 3/10... Batch: 306/306... Train Loss: 0.232... Train Accruacy: 0.595... Val Accuracy: 0.619 Epoch time : 0:03:33.580350\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 4/10... Batch: 306/306... Train Loss: 0.220... Train Accruacy: 0.609... Val Accuracy: 0.628 Epoch time : 0:03:36.363818\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 5/10... Batch: 306/306... Train Loss: 0.220... Train Accruacy: 0.620... Val Accuracy: 0.635 Epoch time : 0:03:38.459146\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 6/10... Batch: 306/306... Train Loss: 0.217... Train Accruacy: 0.627... Val Accuracy: 0.637 Epoch time : 0:03:33.835509\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 7/10... Batch: 306/306... Train Loss: 0.210... Train Accruacy: 0.635... Val Accuracy: 0.644 Epoch time : 0:03:29.653665\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 8/10... Batch: 306/306... Train Loss: 0.212... Train Accruacy: 0.640... Val Accuracy: 0.646 Epoch time : 0:03:31.372320\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 9/10... Batch: 306/306... Train Loss: 0.206... Train Accruacy: 0.646... Val Accuracy: 0.650 Epoch time : 0:03:29.890247\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 10/10... Batch: 306/306... Train Loss: 0.204... Train Accruacy: 0.651... Val Accuracy: 0.658 Epoch time : 0:03:27.909774\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    build_and_train_network_all_in_one(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, x_train, x_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train_x, val_x, train_y, val_y):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(train_x)//batch_size\n",
    "        for e in range(epochs):\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            state = sess.run(initial_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "                print(ii,end=' ')\n",
    "                feed = {inputs_: x,\n",
    "                        labels_: y[:, None],\n",
    "                        keep_prob_: keep_prob,\n",
    "                        initial_state: state}\n",
    "                loss_, state, _,  batch_acc = sess.run([loss, final_state, optimizer, accuracy], feed_dict=feed)\n",
    "                train_acc.append(batch_acc)\n",
    "                \n",
    "                if (ii ) % n_batches == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "                    for xx, yy in get_batches(val_x, val_y, batch_size):\n",
    "                        feed = {inputs_: xx,\n",
    "                                labels_: yy[:, None],\n",
    "                                keep_prob_: 1,\n",
    "                                initial_state: val_state}\n",
    "                        val_batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                        val_acc.append(val_batch_acc)\n",
    "                    \n",
    "                    stop_time = datetime.now()\n",
    "                    print()\n",
    "                    print()\n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                          \"Batch: {}/{}...\".format(ii, n_batches),\n",
    "                          \"Train Loss: {:.3f}...\".format(loss_),\n",
    "                          \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "                          \"Val Accuracy: {:.3f}\".format(np.mean(val_acc)),\n",
    "                          \"Epoch time : {}\".format(str(stop_time-start_time)))\n",
    "                    print()\n",
    "                    print()\n",
    "        #saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inputs and Hyperparameters\n",
    "lstm_sizes = [128, 64]\n",
    "vocab_size = len(word_idx) + 1 #add one for padding\n",
    "embed_size = 300\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    build_and_train_network_all_in_one(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, x_train, x_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL FUNCTION IN ONE TRAINING_SIDD\n",
    "\n",
    "def model_train_all_in_one(x_train,y_train,x_val,y_val,vocab_size,\n",
    "                embed_size=300,lstm_neurons_li=[128,64],\n",
    "                keep_prob=0.5,learning_rate=1e-1,epochs=50,batch_size=256):\n",
    "    \n",
    "    # reset default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    ## create placeholder\n",
    "#     x_ph,y_ph,keep_prob_ph = create_placeholders()    \n",
    "    with tf.variable_scope('Placeholders'):\n",
    "        x_ph = tf.placeholder(tf.int32,[None,None],name ='x_ph')\n",
    "        y_ph = tf.placeholder(tf.int32,None, name='y_ph')\n",
    "        keep_prob_ph = tf.placeholder(tf.float32,name='keep_prob_ph')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ## forward propogation\n",
    "#     initial_state,a_output,cell,final_state = forward_propagation(x_ph,vocab_size,\n",
    "#                                                                        embed_size,lstm_neurons_li,\n",
    "#                                                                        keep_prob_ph,batch_size)\n",
    "\n",
    "    # creating embedding layer\n",
    "    with tf.variable_scope('Embedding_layer'):\n",
    "        embedding = tf.Variable(tf.random_uniform((vocab_size,embed_size),minval=-1,maxval=1))\n",
    "        embed_layer = tf.nn.embedding_lookup(embedding,x_ph)\n",
    "    \n",
    "    # creating LSTM layer\n",
    "    with tf.variable_scope('LSTM_layer'):\n",
    "        \n",
    "        # creating lstm cells\n",
    "        lstms = [tf.contrib.rnn.BasicLSTMCell(size,name='lstm_cell') for size in lstm_neurons_li]\n",
    "        # adding dopout to the cells\n",
    "        drops = [tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob = keep_prob_ph) for lstm in lstms]\n",
    "        # stacking multiple LSTM layers\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "\n",
    "\n",
    "        # getting initial state of all zeros\n",
    "        init_state = cell.zero_state(batch_size,tf.float32)\n",
    "        #init_state = tf.identity(init_state, name=\"init_state\")\n",
    "\n",
    "        lstm_outputs,final_state = tf.nn.dynamic_rnn(cell,embed_layer,initial_state=init_state)\n",
    "\n",
    "    # creating sigmoid fc layer\n",
    "    with tf.variable_scope('FC_layer'):\n",
    "        a_output = tf.contrib.layers.fully_connected(lstm_outputs[:,-1],1,activation_fn=tf.sigmoid)\n",
    "        \n",
    "        tf.summary.histogram('Predictions',a_output)\n",
    "    \n",
    "    ## cost calculation\n",
    "#     cost = compute_cost(a_output,y_ph)\n",
    "    with tf.variable_scope('Loss'):\n",
    "        cost = tf.losses.mean_squared_error(y_ph,a_output)\n",
    "        \n",
    "        tf.summary.scalar(\"Loss\", cost)\n",
    "        \n",
    "    ## optimizers\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    ## accuracy definition\n",
    "#     accuracy = acc_fn(a_output,y_ph)\n",
    "    with tf.variable_scope('Accuracy'):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(a_output),tf.int32),y_ph)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "        \n",
    "        tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print(\"Printing all trainable tensor_names\".center(50,'-'))\n",
    "    \n",
    "    print()\n",
    "    print(\"LSTM WEIGHTS\")\n",
    "    print()\n",
    "    \n",
    "    [print(n.name)for n in tf.trainable_variables('LSTM_layer')]\n",
    "    \n",
    "    [tf.summary.histogram(n.name, n)for n in tf.trainable_variables('LSTM_layer')]\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"FC WIEGHTS\")\n",
    "    print()\n",
    "    \n",
    "    [print(n.name)for n in tf.trainable_variables('FC_layer')]\n",
    "    \n",
    "    [tf.summary.histogram(n.name, n)for n in tf.trainable_variables('FC_layer')]\n",
    "    \n",
    "    print()\n",
    "    print(\"tensor_names\".center(50,'-'))\n",
    "    \n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(x_train)//batch_size    \n",
    "        \n",
    "        writer_train = tf.summary.FileWriter('./tb/sidd_all_in_one/train/',sess.graph)\n",
    "        writer_val = tf.summary.FileWriter('./tb/sidd_all_in_one/val/')\n",
    "        \n",
    "        global_step = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            startime = datetime.now()\n",
    "            state = sess.run(init_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            for step,(x,y) in enumerate(get_batches(x_train,y_train,batch_size),1):\n",
    "                \n",
    "                global_step+=1\n",
    "                \n",
    "                print(step,end=' ')\n",
    "                \n",
    "                feed = {x_ph:x,\n",
    "                        y_ph:y,\n",
    "                        keep_prob_ph:keep_prob,\n",
    "                        init_state:state}\n",
    "                \n",
    "                loss_,state,_,batch_acc, s = sess.run([cost,final_state,optimizer,accuracy,summ],feed_dict=feed)\n",
    "                \n",
    "                writer_train.add_summary(s,global_step) # writing summary real time\n",
    "                \n",
    "                train_acc.append(batch_acc)\n",
    "                \n",
    "                \n",
    "                if (step) % 17 == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    #val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "                    \n",
    "                    for xx, yy in get_batches(x_val, y_val, batch_size):\n",
    "                        feed_val = {x_ph: xx,\n",
    "                                y_ph: yy[:,None],\n",
    "                                keep_prob_ph: 1}  \n",
    "                            #initial_state: val_state}\n",
    "\n",
    "                        val_batch_acc,s = sess.run([accuracy, summ], feed_dict=feed_val)                                                                \n",
    "                        val_acc.append(val_batch_acc)\n",
    "                        \n",
    "                    writer_val.add_summary(s,global_step) # writing summary real time\n",
    "                    \n",
    "                # after the last batch is used for training i.e. after every epoch of training, evaluating result\n",
    "                if (step)%n_batches == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    \n",
    "                    #val_state = sess.run(cell.zero_state(batch_size,tf.float32))\n",
    "                    \n",
    "                    for xx,yy in get_batches(x_val,y_val,batch_size):\n",
    "                        feed_val = {x_ph:xx,\n",
    "                                y_ph:yy,\n",
    "                                keep_prob_ph:1}  \n",
    "                                \n",
    "                        \n",
    "                        #val_batch_acc,val_state = sess.run([accuracy,final_state],feed_dict=feed_val)\n",
    "                        val_batch_acc = sess.run([accuracy],feed_dict=feed_val)                        \n",
    "                        \n",
    "                        val_acc.append(val_batch_acc)\n",
    "                    \n",
    "                    stoptime = datetime.now()\n",
    "                    print()\n",
    "                    print()\n",
    "                    print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                          \"Batch: {}/{}...\".format(step, n_batches),\n",
    "                          \"Train Loss: {:.3f}...\".format(loss_),\n",
    "                          \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "                          \"Val Accuracy: {:.3f}\".format(np.mean(val_acc)),\n",
    "                          \"Epoch time: {}\".format(str(stoptime-startime)))\n",
    "                    \n",
    "                    print()\n",
    "                    print()\n",
    "#                     writer = tf.summary.FileWriter('./tb/tensorboard_sidd_all_in_one_'+str(epoch+1),sess.graph)\n",
    "#                     writer.close()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Printing all trainable tensor_names--------\n",
      "\n",
      "LSTM WEIGHTS\n",
      "\n",
      "LSTM_layer/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0\n",
      "LSTM_layer/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0\n",
      "LSTM_layer/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0\n",
      "LSTM_layer/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0\n",
      "INFO:tensorflow:Summary name LSTM_layer/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0 is illegal; using LSTM_layer/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name LSTM_layer/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0 is illegal; using LSTM_layer/rnn/multi_rnn_cell/cell_0/lstm_cell/bias_0 instead.\n",
      "INFO:tensorflow:Summary name LSTM_layer/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0 is illegal; using LSTM_layer/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name LSTM_layer/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0 is illegal; using LSTM_layer/rnn/multi_rnn_cell/cell_1/lstm_cell/bias_0 instead.\n",
      "\n",
      "\n",
      "FC WIEGHTS\n",
      "\n",
      "FC_layer/fully_connected/weights:0\n",
      "FC_layer/fully_connected/biases:0\n",
      "INFO:tensorflow:Summary name FC_layer/fully_connected/weights:0 is illegal; using FC_layer/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name FC_layer/fully_connected/biases:0 is illegal; using FC_layer/fully_connected/biases_0 instead.\n",
      "\n",
      "-------------------tensor_names-------------------\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 1/10... Batch: 306/306... Train Loss: 0.249... Train Accruacy: 0.536... Val Accuracy: 0.551 Epoch time: 0:03:37.839856\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 2/10... Batch: 306/306... Train Loss: 0.249... Train Accruacy: 0.543... Val Accuracy: 0.551 Epoch time: 0:03:38.175429\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 3/10... Batch: 306/306... Train Loss: 0.249... Train Accruacy: 0.546... Val Accuracy: 0.552 Epoch time: 0:03:36.895397\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 4/10... Batch: 306/306... Train Loss: 0.248... Train Accruacy: 0.548... Val Accuracy: 0.552 Epoch time: 0:03:31.315471\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 5/10... Batch: 306/306... Train Loss: 0.248... Train Accruacy: 0.549... Val Accuracy: 0.552 Epoch time: 0:03:38.170890\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 6/10... Batch: 306/306... Train Loss: 0.248... Train Accruacy: 0.549... Val Accuracy: 0.552 Epoch time: 0:03:35.390331\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 7/10... Batch: 306/306... Train Loss: 0.248... Train Accruacy: 0.550... Val Accuracy: 0.552 Epoch time: 0:03:34.476113\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 8/10... Batch: 306/306... Train Loss: 0.248... Train Accruacy: 0.551... Val Accuracy: 0.552 Epoch time: 0:03:39.680285\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 9/10... Batch: 306/306... Train Loss: 0.248... Train Accruacy: 0.551... Val Accuracy: 0.552 Epoch time: 0:03:37.378021\n",
      "\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 \n",
      "\n",
      "Epoch: 10/10... Batch: 306/306... Train Loss: 0.248... Train Accruacy: 0.552... Val Accuracy: 0.552 Epoch time: 0:03:35.567330\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_idx)+1\n",
    "\n",
    "model_train_all_in_one(x_train,y_train,x_val,y_val,vocab_size,\n",
    "                embed_size=300,lstm_neurons_li=[128,64],\n",
    "                keep_prob=0.5,learning_rate=1e-1,epochs=10,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_placeholders():\n",
    "#     'creating placeholders'\n",
    "    \n",
    "#     with tf.variable_scope('Placeholders'):\n",
    "#         x_ph = tf.placeholder(tf.int32,[None,None],name ='x_ph')\n",
    "#         y_ph = tf.placeholder(tf.int32,None, name='y_ph')\n",
    "#         keep_prob_ph = tf.placeholder(tf.float32,name='keep_prob_ph')\n",
    "    \n",
    "#     return x_ph,y_ph,keep_prob_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_propagation(x_ph,vocab_size,embed_size,lstm_neurons_li,keep_prob_ph,batch_size):\n",
    "    \n",
    "#     # creating embedding layer\n",
    "#     with tf.variable_scope('Embedding_layer'):\n",
    "#         embedding = tf.Variable(tf.random_uniform((vocab_size,embed_size),minval=-1,maxval=1))\n",
    "#         embed_layer = tf.nn.embedding_lookup(embedding,x_ph)\n",
    "    \n",
    "#     # creating LSTM layer\n",
    "#     with tf.variable_scope('LSTM_layer'):\n",
    "        \n",
    "#         # creating lstm cells\n",
    "#         lstms = [tf.contrib.rnn.BasicLSTMCell(size,name='lstm_cell') for size in lstm_neurons_li]\n",
    "#         # adding dopout to the cells\n",
    "#         drops = [tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob = keep_prob_ph) for lstm in lstms]\n",
    "#         # stacking multiple LSTM layers\n",
    "#         cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "\n",
    "\n",
    "#         # getting initial state of all zeros\n",
    "#         initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "#         #init_state = tf.identity(init_state, name=\"init_state\")\n",
    "\n",
    "#         lstm_outputs,final_state = tf.nn.dynamic_rnn(cell,embed_layer,initial_state=initial_state)\n",
    "\n",
    "#     # creating sigmoid fc layer\n",
    "#     with tf.variable_scope('FC_layer'):\n",
    "#         a_output = tf.contrib.layers.fully_connected(lstm_outputs[:,-1],1,activation_fn=tf.sigmoid)\n",
    "    \n",
    "#     return initial_state,a_output,cell,final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_cost(a_output,y_ph):\n",
    "    \n",
    "#     with tf.variable_scope('Loss'):\n",
    "#         cost = tf.losses.mean_squared_error(y_ph,a_output)\n",
    "    \n",
    "#     return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # accuracy function\n",
    "\n",
    "# def acc_fn(a_output,y_ph):\n",
    "    \n",
    "#     with tf.variable_scope('Accuracy'):\n",
    "#         correct_pred = tf.equal(tf.cast(tf.round(a_output),tf.int32),y_ph)\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batches(x,y,batch_size = 100):\n",
    "    \n",
    "#     n_batches = len(x)//batch_size\n",
    "    \n",
    "#     # removing left out records\n",
    "#     x,y = x[:n_batches*batch_size],y[:n_batches*batch_size]\n",
    "    \n",
    "#     for i in range(0,len(x),batch_size):\n",
    "#         yield x[i:i+batch_size],y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_training\n",
    "\n",
    "# def model_train(x_train,y_train,x_val,y_val,vocab_size,\n",
    "#                 embed_size=300,lstm_neurons_li=[128,64],\n",
    "#                 keep_prob=0.5,learning_rate=1e-1,epochs=50,batch_size=256):\n",
    "    \n",
    "#     # reset default graph\n",
    "#     tf.reset_default_graph()\n",
    "    \n",
    "#     # create placeholder\n",
    "#     x_ph,y_ph,keep_prob_ph = create_placeholders()\n",
    "    \n",
    "#     # forward propogation\n",
    "#     initial_state,a_output,cell,final_state = forward_propagation(x_ph,vocab_size,\n",
    "#                                                                        embed_size,lstm_neurons_li,\n",
    "#                                                                        keep_prob_ph,batch_size)    \n",
    "    \n",
    "#     # cost calculation\n",
    "#     cost = compute_cost(a_output,y_ph)\n",
    "    \n",
    "#     # optimizers\n",
    "#     optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "#     # accuracy definition\n",
    "#     accuracy = acc_fn(a_output,y_ph)\n",
    "    \n",
    "#     saver = tf.train.Saver()\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         n_batches = len(x_train)//batch_size\n",
    "        \n",
    "#         #writer = tf.summary.FileWriter('./tensorboard_sidd_pre',sess.graph)\n",
    "#         #writer.close()\n",
    "        \n",
    "        \n",
    "#         for epoch in range(epochs):\n",
    "#             startime = datetime.now()\n",
    "#             state = sess.run(initial_state)\n",
    "            \n",
    "#             train_acc = []\n",
    "#             for step,(x,y) in enumerate(get_batches(x_train,y_train,batch_size),1):\n",
    "#                 print('.',end=' ')\n",
    "#                 feed = {x_ph:x,\n",
    "#                         y_ph:y,\n",
    "#                         keep_prob_ph:keep_prob,\n",
    "#                         initial_state:state}\n",
    "                \n",
    "#                 loss_,state,_,batch_acc = sess.run([cost,final_state,optimizer,accuracy],feed_dict=feed)\n",
    "#                 train_acc.append(batch_acc)\n",
    "                \n",
    "#                 # after the last batch is used for training i.e. after every epoch of training, evaluating result\n",
    "#                 if (step)%n_batches == 0:\n",
    "                    \n",
    "#                     val_acc = []\n",
    "                    \n",
    "#                     #val_state = sess.run(cell.zero_state(batch_size,tf.float32))\n",
    "                    \n",
    "#                     for xx,yy in get_batches(x_val,y_val,batch_size):\n",
    "#                         feed_val = {x_ph:xx,\n",
    "#                                 y_ph:yy,\n",
    "#                                 keep_prob_ph:1}  #,\n",
    "#                                 #init_state:val_state}\n",
    "                        \n",
    "#                         #val_batch_acc,val_state = sess.run([accuracy,final_state],feed_dict=feed_val)\n",
    "#                         val_batch_acc = sess.run([accuracy],feed_dict=feed_val)\n",
    "#                         val_acc.append(val_batch_acc)\n",
    "                    \n",
    "#                     stoptime = datetime.now()\n",
    "#                     print()\n",
    "#                     print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "#                           \"Batch: {}/{}...\".format(step, n_batches),\n",
    "#                           \"Train Loss: {:.3f}...\".format(loss_),\n",
    "#                           \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "#                           \"Val Accuracy: {:.3f}\".format(np.mean(val_acc)),\n",
    "#                           \"Epoch time: {}\".format(str(stoptime-startime)))\n",
    "            \n",
    "#             #saver.save(sess,'./model_save3/sentiment.ckpt',global_step = epoch+1)\n",
    "        \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(word_idx)+1\n",
    "\n",
    "# model_train(x_train,y_train,x_val,y_val,vocab_size,\n",
    "#                 embed_size=300,lstm_neurons_li=[128,64],\n",
    "#                 keep_prob=0.5,learning_rate=1e-1,epochs=50,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_test(model_dir,x_test,y_test,batch_size = 256):\n",
    "    \n",
    "#     test_acc = []\n",
    "#     with tf.Session() as sess:\n",
    "#         saver = tf.train.import_meta_graph(model_dir+'.meta')\n",
    "#         saver.restore(sess,model_dir)\n",
    "        \n",
    "#         graph = tf.get_default_graph()\n",
    "                \n",
    "#         y_ph = graph.get_tensor_by_name('Placeholders/y_ph:0')\n",
    "#         x_ph = graph.get_tensor_by_name('Placeholders/x_ph:0')\n",
    "#         keep_prob_ph = graph.get_tensor_by_name('Placeholders/keep_prob_ph:0')\n",
    "#         accuracy = graph.get_tensor_by_name('Accuracy/Mean:0')\n",
    "        \n",
    "#         for i,(x,y) in enumerate(get_batches(x_test,y_test,batch_size),1):\n",
    "#             feed_test = {x_ph:x,\n",
    "#                          y_ph:y,\n",
    "#                          keep_prob_ph:1}                         \n",
    "            \n",
    "#             batch_acc = sess.run([accuracy],feed_dict=feed_test)\n",
    "#             test_acc.append(batch_acc)\n",
    "#         print(\"Test Accuracy : {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = 'model_save/sentiment.ckpt-1'\n",
    "# model_test(model_dir,x_test,y_test,batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
